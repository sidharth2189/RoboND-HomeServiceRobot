Summary of Tasks
In this project, simulatneous localization and mapping and path planning is used to create a home service robot.

#######################################################################################
Official ROS packages
Import these packages now and install them in the src directory of your catkin workspace. Be sure to clone the full GitHub 
directory and not just the package itself.

1) gmapping:(opens in a new tab) With the gmapping_demo.launch file, you can easily perform SLAM and build a map of the 
environment with a robot equipped with laser range finder sensors or RGB-D cameras.
2) turtlebot_teleop:(opens in a new tab) With the keyboard_teleop.launch file, you can manually control a robot using keyboard 
commands.
3) turtlebot_rviz_launchers:(opens in a new tab) With the view_navigation.launch file, you can load a preconfigured rviz 
workspace. You’ll save a lot of time by launching this file, because it will automatically load the robot model, trajectories, 
and map for you.
4) turtlebot_gazebo:(opens in a new tab) With the turtlebot_world.launch you can deploy a turtlebot in a gazebo environment 
by linking the world file to it.

#######################################################################################
Your Packages and Directories
You’ll install these packages and create the directories as you go through the project.

1) map: Inside this directory, store gazebo world file and the map generated from SLAM.
2) scripts: Inside this directory, store your shell scripts.
3) rvizConfig: Inside this directory, store customized rviz configuration files.
4) pick_objects: Write a node that commands robot to drive to the pickup and drop off zones.
5) add_markers: Write a node that model the object with a marker in rviz.

#######################################################################################
Steps
1) SLAM Testing:
The next task of this project is to autonomously map the environment you designed earlier with the Building Editor in Gazebo. 
But before you tackle autonomous mapping, it’s important to test if you are able to manually perform SLAM by teleoperating 
your robot. The goal of this step is to manually test SLAM.

Write a shell script test_slam.sh that will deploy a turtlebot inside your environment, control it with keyboard commands, 
interface it with a SLAM package, and visualize the map in rviz. We will be using turtlebot for this project but feel free 
to use your personalized robot to make your project stand out!

Launch your test_slam.sh file, search for the xterminal running the keyboard_teleopnode, and start controlling your robot.

2) Localization and Navigation Testing
The next task of this project is to pick two different goals and test your robot's ability to reach them and orient itself 
with respect to them. We will refer to these goals as the pickup and drop off zones. This section is only for testing 
purposes to make sure our robot is able to reach these positions before autonomously commanding it to travel towards them.

We will be using the ROS Navigation stack, which is based on the Dijkstra's, a variant of the Uniform Cost Search algorithm, 
to plan our robot trajectory from start to goal position. The ROS navigation stack permits your robot to avoid any obstacle 
on its path by re-planning a new trajectory once your robot encounters them. You are familiar with this navigation stack 
from the localization project where you interfaced with it and sent a specific goal for your robot to reach while localizing 
itself with AMCL. 

Once you launch all the nodes, you will initially see the particles around your robot, which means that AMCL recognizes the 
initial robot pose. Now, manually point out to two different goals, one at a time, and direct your robot to reach them and 
orient itself with respect to them.

3) Reaching Multiple Goals
Earlier, you tested your robot capabilities in reaching multiple goals by manually commanding it to travel with the 2D 
NAV Goal arrow in rviz. Now, you will write a node that will communicate with the ROS navigation stack and autonomously 
send successive goals for your robot to reach. As mentioned earlier, the ROS navigation stack creates a path for your 
robot based on Dijkstra's algorithm, a variant of the Uniform Cost Search algorithm, while avoiding obstacles on its path.

There is an official ROS tutorial that teaches you how to send a single goal position and orientation to the navigation 
stack. You are already familiar with this code from the Localization project where you used it to send your robot to a 
pre-defined goal. Check out the tutorial(opens in a new tab) and go through its documentation.

You will need to modify this code and edit its node name to pick_objects. Then, edit the frame_id to map, since your fixed 
frame is the map and not base_link. After that, you will need to modify the code and include an extra goal position and 
orientation for your robot to reach.

The first goal should be your desired pickup goal and the second goal should be your desired drop off goal. The robot has 
to travel to the desired pickup zone, display a message that it reached its destination, wait 5 seconds, travel to the 
desired drop off zone, and display a message that it reached the drop off zone.

4) Modeling Virtual Objects
The final task of this project is to model a virtual object with markers in rviz. The virtual object is the one being 
picked and delivered by the robot, thus it should first appear in its pickup zone, and then in its drop off zone once 
the robot reaches it.

First, let’s see how markers can be drawn in rviz. Luckily, there’s an official ROS tutorial that teaches you how to 
do it. The tutorial is an excellent reference and includes a C++ node capable of drawing basic shapes like arrows, cubes, 
cylinders, and spheres in rviz. You will learn how to define a marker, scale it, define its position and orientation, 
and finally publish it to rviz. The node included in the tutorial will publish a different shape each second at the same 
position and orientation. Check out the tutorial(opens in a new tab) and go through the documentation to get started.

You will need to first run this node and visualize the markers in rviz. Then you’ll need to modify the code and publish 
a single shape example: a cube. Your code should follow this algorithm:

Publish the marker at the pickup zone
Pause 5 seconds
Hide the marker
Pause 5 seconds
Publish the marker at the drop off zone
Later you will be able to combine this node with the pick_objects node coded earlier to simulate the full home service robot.

5) Now it’s time to simulate a full home service robot capable of navigating to pick up and deliver virtual objects. 
To do so, the add_markers and pick_objects node should be communicating. Or, more precisely, the add_markers node should 
subscribe to your odometry to keep track of your robot pose.

Modify the add_markers node as follows:

- Initially show the marker at the pickup zone
- Hide the marker once your robot reaches the pickup zone
- Wait 5 seconds to simulate a pickup
- Show the marker at the drop off zone once your robot reaches it

There are many ways to solve this problem. To establish communications between the robot and the markers, one method 
already mentioned is to let your add_markers node subscribe to your robot odometry and keep track of your robot pose.

Other solutions to this problem might be to use ROS parameters(opens in a new tab), subscribe to the AMCL pose, or even 
to publish a new variable that indicates whether or not your robot is at the pickup or drop off zone. Feel free to solve 
this problem in any way you wish.